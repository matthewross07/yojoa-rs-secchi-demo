---
title: "xgboost algorithm for Secchi"
author: "B Steele w/Edits from Matt Ross"
date: "2023-05-25"
output: html_document
editor_options: 
  markdown: 
    wrap: 72
---

```{r}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE, comment = FALSE, message = FALSE,
                      cache = TRUE)

library(tidyverse)
library(xgboost)
library(Metrics)
library(ggpmisc)

match_dir = 'data/matchups/'
model_dir = 'data/models/'
```

# Purpose

The original purpose of this script is to apply applying the `xgboost`
algorithm to Remote Sensing Imagery of Lake Yojoa in Honduras, to
estimate Yojoa water clarity. You can read more about this lake
[here](https://www.sciencedirect.com/science/article/pii/S0048969722015479).
We have slightly adopted the code to become a teaching demo on how to
use machine learning algorithms. We also use a myriad of climate
covariates from the ERA5 climate data in this analysis.

You can read more about xgboost all over the internet, but I like the
kaggle
[demo](https://www.kaggle.com/code/rtatman/machine-learning-with-xgboost-in-r/notebook)

## Load matchup data

```{r}
#list all the files in the match directory
match = list.files(match_dir)

prepData = function(df) {
  #make a rowid column
  df_prep = df %>% 
    rowid_to_column() %>% 
    mutate(secchi = as.numeric(secchi)) %>% #there's one wonky value in here with two decimal points... dropping from this analysis
    filter(!is.na(secchi))
  
  #Add ratios then trim to needd to columns to speed up run
  df_prep %>% 
    mutate(RN= med_Red_corr/med_Nir_corr,
           BG= med_Blue_corr/med_Green_corr,
           RB= med_Red_corr/med_Blue_corr,
           GB = med_Green_corr/med_Blue_corr)
}


#load the matchup files 
sameDay = read.csv(file.path(match_dir, match[grepl('same', match) & !grepl('us', match)])) %>%
  prepData(.)
oneDay = read.csv(file.path(match_dir, match[grepl('one', match) & !grepl('us', match)])) %>%
  prepData(.)
threeDay = read.csv(file.path(match_dir, match[grepl('three', match) & !grepl('us', match)])) %>%
  prepData(.)
fiveDay = read.csv(file.path(match_dir, match[grepl('five', match) & !grepl('us', match)])) %>%
  prepData(.)


```


We want to predict the `secchi` value in these datasets, so let's set
the `target` as that variable:

```{r}
## Identify our target (value is secchi)
target <- 'secchi'
```

## Quick xgboost run on threeDay matchups

Let's see what happens if we loosen our time restraint and add more
matchups into the mix

### Make test and training sets

For the same day matchup dataset, let's grab 20% of the data as the
'test' set and the remainder as the training set.

```{r}
# Set random seed
set.seed(799)

##Pull 20% as holdout test data
test <- fiveDay %>%
  sample_frac(.2) 

## Remove holdout data
train <- fiveDay %>% filter(!rowid %in% test$rowid) 

hist(train$secchi)
hist(test$secchi)
```

## Add in the met data with the five day matchups

Let's see what happens if we add in the ERA5 met data. For this example,
we'll use the 5-day summaries, meaning we've summarized the met data as
the mean of the previous 5 days. Since we already made the training/test
datasets, let's stick with those, but name new features.

### xgboost on band data and all the 5-day met data

In our dataset, the 5-day met summaries have the suffix '\_5'

```{r}
band_met5_feats <- c('med_Blue_corr', 'med_Green_corr', 'med_Red_corr', 'med_Nir_corr',
                     'RN', 'BG', 'RB','GB',
                     'tot_sol_rad_KJpm2_5', 'max_temp_degK_5', 'min_temp_degK_5',
                     'tot_precip_m_5', 'mean_wind_mps_5')
```

Now we'll format the data

```{r}
## Format it the way xgboost likes
dtrain <- xgb.DMatrix(data = as.matrix(train[,band_met5_feats]), 
                      label = train[,target])
dtest <- xgb.DMatrix(data = as.matrix(test[,band_met5_feats]), 
                     label = test[,target])

```


### Parameter optimization taken from Sam Sillen

```{r}
# 3) Hypertune xgboost parameters and save as 'best_params' 

grid_train <- expand.grid(
  max_depth= c(3,6,8),
  subsample = c(.5,.8,1),
  colsample_bytree= c(.5,.8,1),
  eta = c(0.1, 0.3),
  min_child_weight= c(3,5,7)
)

hypertune_xgboost = function(train,test, grid){
  
  params <- list(booster = "gbtree", objective = 'reg:squarederror', 
                 eta=grid$eta ,max_depth=grid$max_depth, 
                 min_child_weight=grid$min_child_weight,
                 subsample=grid$subsample, 
                 colsample_bytree=grid$colsample_bytree)
  
  xgb.naive <- xgb.train(params = params, data = dtrain, nrounds = 1000, 
                         watchlist = list(train = dtrain, val = dtest), 
                         verbose = 0,
                         early_stopping_rounds = 20)
  
  
  summary <- grid %>% mutate(val_loss = xgb.naive$best_score, best_message = xgb.naive$best_msg)
  
  return(summary) 
}


## Hypertune xgboost
xgboost_hypertune <- grid_train %>%
  pmap_dfr(function(...) {
    current <- tibble(...)
    hypertune_xgboost(dtrain,dtest,current)
  })

View(xgboost_hypertune)

best_params <- xgboost_hypertune[xgboost_hypertune$val_loss==min(xgboost_hypertune$val_loss),]

best_params
best_params <- list(booster = "gbtree", 
                    objective = 'reg:squarederror',
               eta=best_params$eta,
               max_depth=best_params$max_depth, 
               min_child_weight=best_params$min_child_weight, 
               subsample=best_params$subsample, 
               colsample_bytree=best_params$colsample_bytree)



best_params
```



### Parameter tuning


```{r, include = F}


#run the boost algo with those settings
optimized_booster <- xgb.train(params = best_params, data = dtrain, nrounds = 1000, 
                         watchlist = list(train = dtrain, val = dtest), 
                         print_every_n =100,
                         early_stopping_rounds = 20)




preds <- test %>% 
  mutate(pred_secchi = predict(optimized_booster, dtest))



evals <- preds %>%
  summarise(rmse = rmse(secchi, pred_secchi),
            mae = mae(secchi, pred_secchi),
            mape = mape(secchi, pred_secchi),
            bias = bias(secchi, pred_secchi),
            p.bias = percent_bias(secchi, pred_secchi),
            smape = smape(secchi, pred_secchi),
            r2 = cor(secchi, pred_secchi)^2) 


evals

```

And let's visualize the predictions:

```{r}
ggplot(preds, aes(x = secchi, y = pred_secchi)) + 
  geom_point() +
  geom_abline(color = 'grey', lty = 2) + 
  coord_cartesian(xlim = c(0, 6.5),
                  ylim = c(0,6.5)) +
  stat_poly_eq(aes(label = paste(after_stat(adj.rr.label))),
               formula = y~x, 
               parse = TRUE, 
               label.y = Inf, 
               vjust = 1.3) +
  labs(title = 'Quick xgboost - Yojoa Secchi\nfive day matchups, band and 5-day met summaries', 
       subtitle = 'Grey dashed line is 1:1', 
       x = 'Actual Secchi (m)', 
       y = 'Predicted Secchi (m)')  +
  theme_bw()+
  theme(plot.title = element_text(hjust = 0.5, face = 'bold'),
        plot.subtitle = element_text(hjust = 0.5))
```


### Applying model to full data


```{r}
full_stack <- read_csv('data/upstreamRS/yojoa_corr_rrs_met_v2023-04-17.csv') %>%
  mutate(secchi = 100) %>%
  prepData(.) 




stack_xgb <- xgb.DMatrix(data = as.matrix(full_stack[,band_met5_feats]))


full_stack_simp <- full_stack %>%
  mutate(secchi = predict(optimized_booster, stack_xgb)) %>%
  select(date, location, secchi, mission) %>%
  mutate(date = mdy)

situ_stack <- read_csv('data/in-situ/Secchi_completedataset.csv') %>%
  mutate(secchi = as.numeric(secchi),
         date = mdy(date)) %>%
  filter(!is.na(secchi)) %>%
  mutate(mission = 'Measured') %>%
  bind_rows(full_stack_simp)


library(ggthemes)
ggplot(situ_stack, aes(x = date, y = secchi, color = mission)) + 
  geom_point() + 
  scale_color_few() + 
  theme_few() + 
  facet_wrap(~location)


```

